---
title: "情報収集を全自動化した ― RSS・LLMキュレーション・Discord配信をPythonで組んだ話"
emoji: "📡"
type: "tech"
topics: ["Python", "RSS", "LLM", "Claude", "自動化"]
published: true
---

## Xのタイムラインが情報源のすべてだった

正直に言うと、技術トレンドのキャッチアップはほぼXに頼り切っていた。フォローしているエンジニアのポストを眺めて、バズっているものを拾う。それが自分の情報収集のほぼ全てだった。

しばらくはそれで回っていた。ただ、ある時期から違和感を覚えるようになった。

- **タイムラインが偏る** ― アルゴリズムが自分の関心を強化する方向に働くので、見える世界がどんどん狭くなる。フォロー外の視点が入ってこない
- **バズ＝重要ではない** ― Xで話題になるものと、エンジニアとして本当に押さえるべき技術動向は必ずしも一致しない。インプレッション狙いのポストに判断を引っ張られる
- **英語圏の一次情報に届かない** ― 日本語のXコミュニティで流れてくる頃には、すでに数日遅れていることが多い
- **受動的すぎる** ― たまたまタイムラインを開いたタイミングに左右される。見逃したら二度と流れてこない

Xは速報性と人の温度感を掴むには良いツールだ。ただ、体系的な情報収集の基盤としては脆い。アルゴリズムが作るフィルターバブルの中にいると、自分が何を見落としているかすら分からなくなる。

Xに依存しない情報収集の仕組みを作ろう。そう思って、RSS・LLM・Discord配信を組み合わせたパイプラインをPythonで全自動化した。

数ヶ月運用してみて、Xだけでは絶対に出会えなかった情報に毎日触れられるようになったので、設計と実装をまとめる。

---

## パイプラインの全体像

構築したパイプラインは3つのフェーズで動く。

```
[収集フェーズ]          [キュレーションフェーズ]        [配信フェーズ]

 RSS Feeds ─────┐
                 ├─→ Markdown ─→ LLM(Claude Haiku) ─→ Discord
 GitHub Trend ──┘    ファイル      10〜15件に厳選      サマリー配信
       │
    DeepL API
   (英→日翻訳)
```

1. **収集** ― 複数のRSSフィードとGitHubトレンドから記事を取得。英語記事はDeepL APIで翻訳し、Markdownファイルとして出力する
2. **キュレーション** ― LLMが記事を評価・厳選し、価値の高い10〜15件を選ぶ
3. **配信** ― Discordの自分用サーバーに通知なしで投稿する

すべてPythonで実装し、日次のcron実行で完全に自動化している。

---

## 収集の仕組み ― 複数ソースからの情報集約

### RSSフィードの選定

収集元は、自分の技術スタックと関心領域に合わせて選んでいる。

```python
feeds = [
    # 主要テックメディア
    "https://hnrss.org/newest?points=100",         # Hacker News (100pt以上)
    "https://www.infoq.com/feed/",                  # InfoQ
    "https://techcrunch.com/feed/",                 # TechCrunch

    # AI/LLM特化
    "https://openai.com/news/rss.xml",              # OpenAI News
    "https://rsshub.app/anthropic/news",            # Anthropic (RSSHub経由)

    # 開発ツール・クラウド
    "https://github.blog/feed/",                    # GitHub Blog
    "https://aws.amazon.com/blogs/aws/feed/",       # AWS Blog

    # 日本語ソース
    "https://zenn.dev/feed",                        # Zenn
    "https://b.hatena.ne.jp/hotentry/it.rss",       # はてなブックマーク IT
]
```

ポイントは **hnrss.org** の活用だ。Hacker News本体にはフィルタ付きRSSがないが、hnrss.orgを使えば `?points=100` のようにスコアで足切りでき、ノイズを大幅に減らせる。

AnthropicのようにネイティブなRSSを提供していないサイトには、RSSHubなどのコミュニティツールでフィードを生成する。フィード収集の安定性を考えると、各ソースのRSS提供状況は定期的に確認したほうがいい。

### GitHubトレンドの収集

RSSだけでは拾いきれない「今まさに注目されているリポジトリ」を把握するため、GitHubのトレンドページからも情報を引いている。新しいOSSツールやライブラリの早期発見に役立つ。

### DeepL APIによる翻訳

一次情報の多くは英語だ。疲れていても読めるように、英語記事はDeepL APIで日本語に翻訳している。

```python
def translate_if_needed(article):
    if detect_language(article.title) == "en":
        article.title_ja = deepl_translate(article.title, target="JA")
        article.summary_ja = deepl_translate(article.summary, target="JA")
    return article
```

DeepL APIを選んだのは、技術文書の翻訳精度が高く専門用語の扱いが自然だからだ。DeepLの次世代言語モデルはブラインドテストでGPT-4比1.7倍、Google翻訳比1.3倍の品質評価を得ている。

翻訳にLLMを使う選択肢も検討した。しかしコストとレイテンシの両面でDeepLが勝った。API Freeプランは月50万文字まで無料。日次パイプラインの翻訳量ならFreeプランの範囲内で収まる。

### Markdownへの出力

収集した記事はMarkdownファイルとして保存する。日付ごとにファイルが生成され、次のキュレーションフェーズへの入力になる。

```markdown
## 2026-02-17 収集記事一覧

### [1] Anthropic Releases New Model with Enhanced Coding Capabilities
- URL: https://example.com/article1
- ソース: Anthropic
- 翻訳タイトル: Anthropic、コーディング能力を強化した新モデルを発表
- サマリー: ...
```

---

## LLMキュレーション ― AIが読むべき記事を選ぶ

### なぜキュレーションが必要か

RSSフィードから集まる記事は、1日あたり数十件から百件を超えることもある。これを全部流すと、情報過多で結局読まなくなる。

実際に試した。キュレーションなしでそのまま流したところ、1週間で見なくなった。RSSリーダーの未読が溜まるのと同じ現象が再現されただけだった。

かといって毎日手作業で選定するのは続かない。キーワードフィルタでは文脈を踏まえた取捨ができない。そこでLLMによるキュレーションを導入した。

### 選定基準の設計

LLMに渡すプロンプトでは、選定基準を明確に書いている。

```python
curation_prompt = """
以下の記事リストから、ソフトウェアエンジニアにとって価値の高い記事を
10〜15件選定してください。

## 選定基準（優先度順）

1. AI/LLM動向: 新モデル、新サービス、実用的なユースケース
2. 開発ツール: 生産性を向上させるツールやサービスのアップデート
3. SaaS/クラウド: 主要クラウドプロバイダーの新機能、料金変更
4. GitHubリポジトリ: 実用的で注目度の高いOSSプロジェクト

## 除外基準

- 初心者向けチュートリアル
- 広告色の強いプレスリリース
- 古い情報の焼き直し

## 出力形式
選定した記事ごとに、選定理由を1行で添えてください。
"""
```

重視したのは除外基準の明示だ。LLMは指示がなければ記事を落とすことに慎重になりがちで、件数が膨らむ。除外条件を具体的に書くことで、厳選の精度が上がった。

### Claude Haikuを採用した理由

キュレーションにはClaude 3 Haikuを使っている。

- **コストが低い** ― 日次で回すのでランニングコストは重要。Claude 3 Haikuは入力100万トークンあたり0.25ドル
- **タスクに対して十分な精度** ― 記事の選定と要約にはHaikuクラスで十分な品質が出る
- **応答が速い** ― パイプライン全体を高速に回せる

より高性能なモデルを使うことも検討したが、キュレーションは判断の正確さより「大外しをしない安定感」が求められるタスクだ。コスト対効果でHaikuがベストだった。

### ハルシネーション対策 ― 運用で得た教訓

LLMキュレーションで最も注意すべき点がハルシネーションだ。

運用中、ソース情報がURLのみの場合にLLMが記事内容を推測し、実際とは異なるサマリーを生成するケースが発生した。URLのドメインとパス名から「おそらくこういう内容だろう」と補完してしまう。

対策として3つの施策を実施した。

**1. メタデータの事前取得**

URLだけでなく、ページタイトルやog:descriptionを事前にフェッチしてLLMに渡す。これだけでハルシネーション率が大幅に下がった。

```python
def enrich_article_metadata(article):
    """URLからメタデータを取得し、LLMへの入力を充実させる"""
    metadata = fetch_url_metadata(article.url)
    article.title = metadata.get("og:title", article.title)
    article.description = metadata.get("og:description", "")
    return article
```

**2. アンチハルシネーション指示の追加**

プロンプトに「提供された情報のみに基づいて要約してください。推測や補完は行わないでください」と明記する。LLMは指示があれば「分からない」と答えられる。指示がなければ補完しようとする。この差は大きい。

**3. 後処理でのバリデーション**

出力結果に対して、元記事のタイトルやキーワードとの整合性をチェックする。完全な自動検証は難しいが、明らかな逸脱は検出できる。

---

## Discordへの配信設計 ― 日常に溶け込む情報共有

### 通知なし配信という設計判断

配信先は自分用のDiscordサーバーにしている。ここで最も大事にした設計判断は**通知なしで配信する**ことだ。

技術ニュースは今すぐ読む必要がない。手が空いたときに目を通せばいい。通知が鳴るたびに集中が途切れては本末転倒だ。

```python
async def post_to_discord(channel, message):
    await channel.send(
        content=message,
        suppress_embeds=False,
        silent=True  # 通知を抑制
    )
```

discord.py 2.2以降では `silent=True` パラメータが使える。内部的には `MessageFlags.SUPPRESS_NOTIFICATIONS` フラグが設定され、プッシュ通知やデスクトップ通知を抑制する。チャンネル内のメッセージ自体は通常どおり表示される。

### メッセージの構成

配信は2段構成にしている。

**1. サマリーメッセージ**

その日の厳選記事の概要を一覧で示す。忙しい日はこれだけ見ればいい。

```
本日のテックニュース（2026/02/17）12件

1. Bun 1.3リリース — GC改善でアイドルCPU消費を100分の1に削減
2. GitHub Copilot Workspace がGA — AIペアプロの新しい形
3. AWS Step Functions、Express Workflowsの同時実行上限を5倍に拡張
4. shadcn/ui にRTLサポートが追加
...
```

**2. 個別記事メッセージ**

各記事について、LLMが生成した要約と元記事のURLを投稿する。

```
Bun 1.3リリース — GC改善でアイドルCPU消費を100分の1に削減

JavaScriptランタイムBunの1.3がリリースされた。
GCをイベントループに統合し、アイドル時のCPU消費を
100分の1に削減。メモリ使用量も40%減少。
組み込みSQLクライアントやRedisクライアントも追加された。

https://bun.sh/blog/bun-v1.3
```

### レート制限対策

DiscordのAPIにはレート制限がある。短時間に大量のメッセージを送ると制限に引っかかるため、メッセージ間に待機時間を入れている。

```python
for article in selected_articles:
    await post_article_message(channel, article)
    await asyncio.sleep(0.3)  # 300msの間隔を確保
```

0.3秒のインターバルは実運用で安定する最小値だった。10〜15件の配信でも合計5秒以内に完了するため、体感上の遅延はない。

---

## 数ヶ月運用してみて ― Xだけでは見えなかった世界

### 数字で見る変化

| 指標 | 導入前（X依存時代） | 導入後 |
|------|--------|--------|
| 情報ソース | Xのタイムライン中心 | RSS 20+ソース＋GitHubトレンド |
| 英語記事へのアクセス | Xで翻訳ポストを待つ | 一次情報を翻訳付きで毎日取得 |
| 能動的な情報収集の時間 | Xを眺める30分/日 | 厳選記事を読む5分/日 |
| 情報の鮮度 | 日本語圏で話題になってから | 英語圏の公開直後 |

### 一番大きかった変化：フィルターバブルの外に出られた

これが想定以上の効果だった。

Xのタイムラインでは絶対に流れてこなかったであろう記事が、毎日のように届く。自分がフォローしていない分野のOSS、注目していなかったクラウドサービスの新機能、海外で盛り上がっているのに日本語圏ではまだ話題になっていない技術。

Xにいた頃は「自分はちゃんとキャッチアップできている」と思っていた。パイプラインを動かし始めて、それが錯覚だったと気づいた。アルゴリズムが見せてくれる世界は、自分の関心の延長線上にあるものだけだった。

**英語記事のハードルが消えた。** 翻訳済みで流れてくるので、Xで誰かが日本語で要約してくれるのを待つ必要がなくなった。情報の鮮度が数日単位で上がった。

**技術選定の引き出しが増えた。** 新しいOSSやサービスを早い段階で認知できるため、個人開発や業務での技術選定で「あ、これ前にニュースで見たやつだ」という場面が増えた。

**Xとの付き合い方も変わった。** 情報収集の義務感から解放されたことで、Xは純粋に人の意見や温度感を見る場所になった。タイムラインを追わなきゃというプレッシャーがなくなって、むしろ快適に使えている。

---

## つまずきポイントと対処法

### RSSフィードの不安定さ

RSSフィードは提供元の都合で突然フォーマットが変わったり、配信が止まったりする。テック企業のブログはサイトリニューアルでURLごと変わることがある。

フィードごとにエラーハンドリングを入れ、取得失敗時はスキップして他のフィードの処理を続行する設計にした。加えて、フィードの死活監視を週次で走らせ、応答がないフィードを検出できるようにしている。

### LLMの出力形式のブレ

LLMの出力は毎回微妙に異なる。JSONで返すよう指示しても、Markdownで返ってきたり余計な前置きが付いたりする。

パース処理に柔軟性を持たせることで対処した。正規表現でJSONブロックを抽出する処理に加え、箇条書き形式もフォールバックとして受け付ける多段パーサーを用意している。

### 翻訳のコスト管理

DeepL APIの翻訳は高品質だが、長文記事を丸ごと翻訳するとコストが膨らむ。タイトルとサマリー（最大300文字程度）のみを翻訳対象とすることで、品質を保ちつつFreeプランの範囲内に収めている。

---

## まとめ

RSS収集、LLMキュレーション、Discord配信。この3つのフェーズを組み合わせて、情報収集パイプラインを全自動化した。

技術的に特別なことはしていない。RSSパーサー、翻訳API、LLM API、Discord Bot。既存の技術を組み合わせただけだ。ただ、これらを1つのパイプラインとしてつなぎ、日次で安定稼働させることで、Xのタイムラインの外にある情報に毎日触れられるようになった。

Xが悪いという話ではない。速報性と人の温度感を掴むにはXは優れたツールだ。ただ、技術トレンドの体系的なキャッチアップをXだけに頼るのは、アルゴリズムに自分の視野を委ねることと同じだった。

自分の情報源を自分で設計する。それだけで見える世界がかなり変わる。プロトタイプは2日で動くので、Xのタイムラインに物足りなさを感じている人は試してみてほしい。
